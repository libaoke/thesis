\chapter{研究现状和相关工作}
\label{cha:background}

本文的全部工作都是针对竞价型云平台相较传统云平台的新特点对几类典型系统应用的模型和设计加以改进以提升其性能与可用性，在整个研究工作中涉及到多个不同的子领域。本章首先对竞价型云平台的研究现状尤其是竞价型实例应用优化工作进行详细介绍和分析，然后再对本文的研究中所涉及的子领域相关工作加以讨论。

针对竞价型云平台的研究工作主要包括竞价型实例的市场价格预测模型、适应竞价型实例的计算框架设计、在线服务架构等等。竞价型云平台的出现时间比较短，竞价型实例价格动态变化的特点是传统云平台中的应用不曾考虑的。竞价型实例的市场价格预测模型为更好的认识和使用竞价型实例提供了基础支持和帮助。新型的计算资源有着极其诱人的低廉价格，随之而来的是随时可能被云平台回收的不可靠属性。如何利用这些竞价型实例实现效益可观的计算成本节省，同时保证应用的可用性及性能？通常的观点认为：竞价型实例的自身特点决定了它只适合使用在不需要高可用性的场景中。大部分研究工作也是针对可中断、时间相对灵活的计算任务考虑如何利用竞价型实例实现计算成本的最大节约，以及设计应对竞价型实例特有的节点失效的容错手段。但计算成本节约上的巨大经济驱动力推动着竞价型实例向其它场景的普及，已经有一些工作开始尝试利用竞价型实例提供高可用的在线服务。甚至有研究者设计出将不可靠的竞价型实例封装为可靠计算资源的初步方案。

本文还涉及到的多个不同子领域包括：大规模并行任务调度、分布式系统可用性理论、高可用及迁移技术等。这些相关领域的工作有的为本文提供了参考和帮助，有的是本文的方法和技术得以实现的基础。没有了这些 ``巨人的肩膀''，本文的工作也不可能完成。但本文所面临的是充满挑战的新问题，需要在新的背景和环境下重新审视已有方法和技术并作出创造性的改变。本章在讨论这些相关工作时，将结合本文要解决的问题进行详细说明。

\section{竞价型实例的市场价格模型}
考虑到部署在竞价型实例上的计算系统的调度算法和容错机制设计的需要，竞价型实例价格预测是一个相关研究中需要探索的基础问题。竞价策略的设计和竞价型实例的失效模型都离不开对市场价格的预测分析。虽然 Amazon EC2 等云平台并没有披露其底层定价算法的细节，许多研究者在云平台外部对竞价型实例的价格模型进行了深入研究。

Javadi 等人 \cite{Javadi:2011:SMS:2120969.2121740} 对 Amazon EC2 云平台的四个区域一年多的市场价格历史数据做了全面的统计分析。基于对所有配置类型竞价型实例的价格和价格变化间隔时间的详细分析，Javadi 等人认为竞价型实例的市场价格数据有明显的双模态性，而竞价型实例的价格变化间隔则在两小时处形成明显的峰值。而且除了 ``m1.small'' 类型竞价型实例，竞价型实例市场价格的概率分布是几乎对称的。``m1.small'' 类型实例的不同可能是作为 Amazon EC2 云平台中最便宜的计算资源有着同其它类型节点不同的使用模式造成的。据此，他们提出了使用如下的高斯混合分布来拟合竞价型实例的价格和价格变化时间间隔的概率统计模型：
\begin{equation}\label{eq_mog}\nonumber 
CDF(x; k, \vec p, \vec u, \vec{\sigma^2}) = \sum_{i=1}^{k}{\frac{p_i}{2}(1+erf(\frac{x-\mu_i}{\sigma_i\sqrt{2}}))}
\end{equation}

其中 $\vec u$，$\vec{\sigma^2}$，和 $\vec p$ 是 $k$ 个高斯分布的均值、方差和概率的向量。而 $erf$ 是误差函数，定义如下：
\begin{equation}\label{eq_erf}\nonumber 
erf(x) = \frac{2}{\sqrt{\pi}}\int_{0}^{x} e^{-t^2}dt
\end{equation}

通过参数估计和模型校准，该模型在新的价格数据集上的最大相对误差小于 4\%。该模型为深入的理解竞价型实例的市场价格提供了一个良好的基础，但该模型没有考虑用户的竞价对竞价型实例的节点失效的影响。

Ben-Yehuda 等人 \cite{AgmonBen-Yehuda:2013:DAE:2509413.2509416} 推测截止2011年10月之前 Amazon EC2 云平台的竞价型实例的市场价格在大部分时间内并不是市场驱动的，而是由一个云平台内部的基于自回归模型的定价算法确定的。通过对大量竞价型实例历史价格数据的分析，Ben-Yehuda 等人认为在较高的价格时反映了市场变化、在较低的价格时则是决定于一个动态保留价格。该动态保留价格通过一个隐藏的一阶自回归模型随机设定。对于如下一阶自回归过程：
\begin{equation}\label{eq_ar1}\nonumber 
\Delta_i = - a_1\Delta_{i-1}+\epsilon(\sigma)
\end{equation}

2010 年 4 月 到 7 月的竞价型实例价格数据可以拟合的非常好（即：更高阶的 $a_i, i > 1$ 的系数可忽略）。其中，$a_1 = 0.7$，$\epsilon(\sigma)$ 是标准差为 $\sigma$ 的白噪声。令 $F$ 和 $C$ 分别表示人为指定价格范围的下界和上界，$\sigma$ 的拟合结果为 $0.39 (C - F)$。这个拟合结果在大多数实例类型上都得到了准确验证。唯一的例外也是 ``m1.small'' 类型的竞价型实例，该类型下的拟合结果是 $a_i = 0.5$，$\sigma = 0.5 (C - F)$。基于这些分析和数据拟合，Ben-Yehuda 等人构造出的保留价格算法主要步骤如下：
\begin{enumerate}
\item 将保留价格 $P_0$ 初始化为 $F$，价格变化 $\Delta_0$ 初始化为 $0.1 (C - F)$。
\item 下一个保留价格 $P_i$ 由 $P_{i-1} + \Delta_i$ 给出，其中 $\Delta_i = - 0.7 \Delta_{i-1}+\epsilon(0.39 \cdot (C - F))$。
\item 如果 $P_i \notin [F, C]$，则重复执行上一步骤。
\end{enumerate}

F 和 C 根据虚拟机实例类型各不相同，价格最终需要四舍五入保留精度到 0.1 美分。他们的研究工作给竞价型云平台的相关研究尤其是使用竞价型实例的价格数据的研究工作一个非常重要的提示：在对价格数据的分析中一定要区分这部分数据是由云平台的内部机制随机设定的，还是真正的反映了真实的市场供需关系。以 2011 年 10 月之前的竞价型实例价格数据为例，很多价格数据特征（最小值、变动间隔等）表现出明显的人工干预行为。最后他们提醒其它研究者在进行竞价型实例价格预测模型训练时，一定要注意使用的价格历史数据的时间节点。

Chohan 等人 \cite{chohan2010see} 和 Song 等人 \cite{song2012optimal} 通过统计推断确认了 Amazon EC2 云平台的竞价型实例的竞价价格序列满足马尔科夫性，Song 等人指出竞价型实例的价格变化时间间隔并不满足无记忆型。根据 Sewook Wee \cite{5948651} 在 2011 年的分析，竞价型实例市场价格采样累计分布函数约每一个小时有明显的增长。Song 等人在他们的工作中将竞价型实例的历史价格数据被以 1 小时为单位做离散化处理，并使用一个离散化的半马尔可夫（semi-Markovian）链描述竞价型实例的价格波动。

这个马尔可夫链的边是以 1 小时为间隔的价格转移概率。给定概率转移矩阵，可以使用如下的 Chapman-Kolmogorov 等式的变体计算 $n$ 步转移概率：
\begin{equation}\label{eq_ck}\nonumber 
P(i, b, n) = \sum_{j \notin B}{M_{ij}P(j, b, n-1)}
\end{equation}

其中，
\begin{equation}\label{eq_ckcond}\nonumber 
P(i, b, 0) = \begin{cases}
0 &\mbox{if $i \in B$}\\
1 &\mbox{if $i \notin B$}
\end{cases}
\end{equation}

其中，竞价型实例在启动时的初始市场价格为 $i$，经历了 $n$ 个时间单元。超过竞价 $b$ 的价格的集合为 $B$。$M_{ij}$ 是价格点 $i$ 到价格点 $j$ 的转移概率矩阵。$P(i, b, n)$ 可以通过递归的方式求解。最基本的转移概率（第 0 步转移概率）就是判断所设定的竞价能否申请到竞价型实例。

在本文的关于使用竞价型实例的分布式服务的可用性与成本模型的工作中，半马尔可夫（semi-Markovian）链模型被嵌入到竞价型实例的失效模型中。

\section{在竞价型云平台上执行可中断计算任务}
自 Amazon EC2 发布竞价型实例以来，相关领域的研究人员已经表现出了对利用竞价型实例实现低成本计算的极大兴趣。考虑到竞价型实例可能因竞价不足失效的特点，大部分现有工作集中于将竞价型实例用于可中断、时间灵活的计算任务。研究的对象包括主流计算框架 MapReduce、批处理任务、有截止期限或 SLA 要求的任务等。其中有些研究工作主要考虑的是在一个固定竞价下如何解决竞价型实例的频繁失效问题，有些研究工作则基于统计分析和竞价型实例市场价格历史数据设计动态竞价策略。

\subsection{MapReduce 作业}
Chohan 等人 \cite{chohan2010see} 基于对竞价型实例市场价格序列满足马尔科夫性的分析，给定当前的竞价型实例市场价格 $i$，竞价 $b$，最大执行时间单元数 $\tau$，可以得出该竞价型实例的寿命 $l$ 的期望：
\begin{equation}\label{eq_elife}\nonumber 
E(l) = \sum_{n=1}^{\tau}nP(i, b, n)
\end{equation}

此模型可以用于有计划的备份数据减少竞价型实例失效带来的损失，而且可以用于竞价策略保证在相同的计算成本预算下获得更多的竞价型实例。Chohan 等人通过实验证明了使用竞价型实例可以有效加速 MapReduce 任务的执行，对于某些任务可加速超过两倍而计算成本只有 44\%。同时，一些测试中也出现了相比不用竞价型实例变慢了 27\% 的情况。

MapReduce 计算框架中的容错机制可以处理竞价型实例被回收的情况，但可能增加 MapReduce 作业的完成时间。Chohan 等人的工作通过对竞价型实例市场价格的建模和实验验证揭示了使用竞价型实例加速时间灵活、可被中断的计算任务的机会。

对于使用竞价型实例的 Hadoop \cite{Hadoop} 计算框架，一个任务的失败不会导致整个作业的执行失败。失败的任务可以被调度到其它可用的竞价型实例或按需型实例上重新执行。只要将记账任务（bookkeeper）部署在正常的按需型实例上保证他在作业执行期间不间断的运行，整个作业中的所有任务最终会得以调度和完成。然而，Hadoop 等许多 MapReduce 实现中并没有考虑为竞价型云平台作出针对性的设计。为了能真正利用到竞价型实例的低成本优势，Huan Liu \cite{Liu:2011:CMC:2170444.2170450} 在其之前的工作 Cloud MapReduce \cite{Liu:2011:CMM:2007336.2007355} 的基础上提出了 Spot Cloud MapReduce。

Cloud MapReduce 的特点是使用了多种云平台服务，如：Amazon EC2 云平台的 S3 (Simple Storage Service)，SQS (Simple Queue Service)，SimpleDB (Simple Database Service)，大大简化了 MapReduce 计算框架的设计。在 Cloud MapReduce 中，一个 MapReduce 作业的输入文件存放在 S3 中。通过 SQS 创建多个队列（Map 队列，Reduce 队列，Master 队列，Output 队列）用于控制作业进行，Reduce 队列有多个，其它队列各有一个。执行 Map 任务的虚拟机实例从 Map 队列获取要处理的文件位置，从 S3 读取文件，将结果放入 Reduce 队列。执行 Reduce 任务的虚拟机实例从 Master 队列获取要处理的数据位置，从 Reduce 队列读取中间数据，将结果写入 Output 队列。执行 Map 任务和 Reduce 任务的虚拟机实例在完成任务后，向 SimpleDB 提交完成记录，这样通过查询 SimpleDB 就可以判断整个作业进行到哪个阶段、是否完成。

为了应对竞价型实例带来的 Cloud MapReduce 作业的大量工作节点失效问题，Spot Cloud MapReduce 基于 Cloud MapReduce 做出了如下改变：
\begin{enumerate}
\item 修改了 Map 队列中任务划分的格式，除 Map 任务的 id，对应文件位置外，还加入了文件的偏移。对于作业初始划分，文件偏移均为 0。
\item Spot Cloud MapReduce 实现了 Map 任务的流式输出到分级缓冲区，并尽快持久化到 Reduce队列。当竞价型实例被回收时，Spot Cloud MapReduce 利用关机脚本做应急处理，首先停止用户定义 Map 函数的执行，然后将分级缓冲区中的数据提交到 Reduce 队列中。
\item 在任务提交机制上改为允许部分提交，即：当分级缓冲同步到 Reduce 队列后，记录该 Map 任务的对应文件偏移部分已处理完。
\item 最后只要在 SimpleDB 中查询到一个任务的输入文件的所有对应部分，就表示该任务已完成。如有缺少的部分，则重新在 Map 队列中加入新的任务处理该部分输入。
\end{enumerate}

由于 Amazon EC2 的 SQS 不支持 FIFO，Reduce 任务无法实现流式输出到 Output 队列。Spot Cloud MapReduce 只修改了 Map 任务部分的实现，这已经能够在很大程度上保证大量节点同时失效的情况下计算任务继续执行下去并最终得以完成。通过使用竞价型实例，执行 MapReduce 作业的成本将会大大降低。当然，MapReduce 作业的完成时间可能变得很长。

通过直接修改系统本身的实现，Spot Cloud MapReduce 无需借助任务检查点等手段就达到了容错的目的。对于类似的可以直接修改实现的系统，在检查点需要保存的数据量比较大的情况下，这是在利用竞价型实例时应首先考虑的一个更为高效的实现思路。

\subsection{批处理作业}
针对计算密集型的、任务可分的批处理作业，Yi 等人 \cite{Yi:2010:RCS:1844768.1845343, 5975137} 提出了自适应的检查点策略和实例类型切换策略来尽量消除使用竞价型实例带来的不可靠，减少作业执行成本和作业完成时间。检查点技术是将当前应用全部状态的快照写到一个持久化存储设备中，在之后的某个时间点遇到故障时通过使用之前的快照恢复执行的容错机制。Yi 等人研究了在竞价型云平台的场景中何时做检查点最优的策略问题。

在Yi 等人设计的自适应检查点策略中，每十分钟进行一次是否要做检查点的决策。这个决策基于在这之后发生竞价节点失效时的所需预期恢复时间 $R(t)$。可以分别计算在跳过这个检查点和做这个检查点的情况下发生竞价节点失效时所需预期恢复时间 $R_{skip}$，$R_{take}$。如果 $R_{skip} > R_{take}$，则应该做这个检查点，否则应该跳过这个检查点。$R_{skip}$ 和 $R_{take}$ 的预估依赖于基于已有价格历史数据做出的概率统计，用于计算的其它参数有原始竞价价格，已经正常运行的时间，上次检查点到现在的时间，以及现在的竞价型实例市场价格。

令 $t_p$ 表示当前时间点，$t$ 表示上次做检查点距 $t_p$ 的时间长度，$t_r$ 表示完成作业相对 $t_p$ 还需的时间长度，$r$ 表示节点失效后重启一个任务所需的时间开销，$t_c$ 表示做检查点之间的时间间隔，$f(t)$ 表示在给定竞价价格和当前市场价格下经过时间 $t$ 节点的失效概率，$T(w, t_p)$ 表示在没有检查点的情况下，从时间点 $t_p$ 开始完成一个需要在 $w$ 个时间单元不出现节点失效的任务所需的期望执行时间，则有：
\begin{equation}\label{eq_rskip}\nonumber 
R_{skip}(t, t_p) = \sum_{k=0}^{t_r - 1}(k + r + T(t,t_p))f(k+t_p)
\end{equation}
\begin{equation}\label{eq_rtake}\nonumber 
R_{take}(t, t_p) = \sum_{k=0}^{t_r - 1}(k + r)f(k+t_p) + t_c\sum_{k=t_r}^{\infty}f(k+t_p)+T(t,t_p-t)\sum_{k=0}^{t_c - 1}f(k+t_p)
\end{equation}

其中，
\begin{equation}\label{eq_t}\nonumber 
T(w,t_p) = \frac{w\sum_{k=w}^{\infty}f(k+t_p)+\sum_{k=0}^{w-1}(k+r)f(k+t_p)}{1-\sum_{k=0}^{w-1}f(k+t_p)}
\end{equation}

对于可分的任务负载（应用本身可以灵活根据 CPU 核心数运行）面临竞价型实例被回收时，Yi 等人认为可以考虑选择其它不同配置类型的竞价型实例并提出了实例类型切换策略。在Yi 等人提出的节点配置类型切换策略中，主要提供了如下启发式规则：
\begin{enumerate}
\item 最低价：选择按每核价格计算最低的实例类型。直觉上似乎某个类型的竞价型实例市场价格越低，再相同竞价下出现竞价不足的可能越小。
\item 最低失效概率：选择失效概率最低的实例类型。失效概率的分布根据价格数据动态计算，最低失效概率考虑了设定的竞价、竞价型实例的市场价格及变化趋势。这个策略以缩短作业完成时间为目标。
\item 最高失效概率：选择失效概率最高的实例类型。失效概率越高，有更高的概率利用到不足一小时竞价型实例被回收免费的特性。这个策略的目标是减少完成作业的计算成本。
\end{enumerate}

检查点技术是计算任务常用的容错技术手段，节点类型切换策略属于竞价策略的范畴。Yi 等人的研究通过对检查点开销和出现错误的恢复时间代价上的权衡，给出了利用竞价型实例运行批处理作业时一个自适应的检查点执行策略。节点类型切换策略则从减少作业完成时间和减少作业计算成本两个角度给出了不同的启发式规则。他们的工作为如何在易错的竞价型实例上执行批处理作业的用户提供了有益参考，但他们并没有在真实系统中实现这样的策略而是通过仿真加以验证。

Subramanya 等人 \cite{Subramanya:2015:SBC:2806777.2806851} 设计实现了一个竞价型云平台中的批处理计算服务 SpotOn。SpotOn 无需对应用做出修改即可通过多种容错机制和可用区选择策略自动地消除竞价型实例被回收的影响。SpotOn 的原型实现在 Amaon EC2 上，作业被打包进 Linux 容器 \cite{LXC} 以利用高效的进程级检查点和迁移技术。根据批处理作业的预期执行时间和资源使用需要，SpotOn 在所有竞价型及按需型实例市场中选择合适的可用区和容错机制以减少作业预期完成时间。在竞价型实例被回收时，SpotOn 将在另一个可用区继续执行该作业，根据容错机制和作业资源使用的不同造成的时间开销也不同。

SpotOn 可选的容错机制和模型包括：
\begin{enumerate}
\item 被动迁移：当收到竞价型实例回收告警通知时，将作业所在容器迁移到另一个可用区的虚拟机实例上。如果在两分钟告警时间内未能完成迁移，则需在另外的虚拟机实例上重新执行该作业。这个风险的大小同需要迁移的内存和磁盘状态的大小相关。
\item 主动检查点：定期的对作业状态（包括内存和本地存储等）做检查点，这个机制没有对作业的内存工作集和本地磁盘更新大小的限制。但定期的检查点引入了一定的执行开销。
\item 复制执行：在多个可用区的多个竞价型实例执行同一个作业。复制执行的优势是不受作业的内存工作集和本地磁盘更新大小限制，并且支持并行作业。但仍然存在所有副本节点同时被回收的可能，另一个演化出的策略是在按需型实例上同时备份多个竞价型实例上执行的作业（只备份一个竞价型实例上的作业则没有使用竞价型实例的意义）。虽然这会导致在按需型实例上执行的作业副本进度变慢，但在某一个竞价型实例失效时避免了从头开始执行作业。
\end{enumerate}

SpotOn 首先根据作业特点和用户需求过滤掉不可行的容错机制，再根据各个可用区市场历史价格数据计算出不同的容错机制在各个可用区的计算成本期望 $E(C_k)$ 和作业完成时间期望 $E(T_k)$，然后选择 $\frac{E(C_k)}{E(T_k)}$ 最小的容错机制和可用区。在竞价策略上，Spoton 会确保该作业使用的竞价型实例的竞价总和不超过使用相应按需型实例的价格。

相比已有工作，Subramanya 等人给出了更丰富的批处理作业容错方式，同时实现了一个原型系统，验证了各种容错机制的可行性。采用虚拟容器的方式也保证了该方法广泛的适用性。

\subsection{有 SLA 约束的任务}
在提交某些计算任务时，用户可能还有对截止期限的要求，以及其它的 SLA 约束，如：计算成本预算。对于这类有 SLA 要求且为计算密集型、易并行、可分的任务在竞价型实例上的执行，Andrzejak 等人 \cite{Andrzejak:2010:DMC:1906481.1906533} 使用一些随机变量描述这类计算任务的执行模型：
\begin{enumerate}
\item 执行时间 $ET$：完成计算任务所需时间（包括被中断的时间）
\item 可用时间 $AT$：竞价型实例在任务执行过程中可用的时间
\item 价格期望 $EP$：竞价型实例在可用阶段平均每小时收取的费用。$EP \leq u_b$ 总是成立，$u_b$ 为用户指定竞价。
\item 计算成本 $M$：用户为每个竞价型实例支付的费用，$M = EP \cdot AT$
\end{enumerate}

一些可能的 SLA 约束如下：
\begin{enumerate}
\item 计算成本预算 $B$：每个竞价型实例所花的成本上限；且用户期望满足这个约束的可信系数达到 $C_B$
\item 截止期限 $t_{dead}$：完成任务所需执行时间的上限；且用户期望满足这个约束的可信系数达到 $C_{dead}$
\end{enumerate}

通过使用竞价型实例的历史价格数据预先计算各个随机变量的累积概率分布，可以根据如下模型给出可行的可用区选择和竞价决策：
\begin{enumerate}
\item 计算成本预算约束以可信系数 $C_B$ 被满足 $\iff B \geq M(C_B)$
\item 截止期限约束以可信系数 $C_{dead}$ 被满足 $\iff t_{dead} \geq ET(C_dead)$
\end{enumerate}

Andrzejak 等人提供了对有 SLA 约束的计算任务在竞价型实例上执行的一个简单思路，但由于所有随机变量的概率分布都是预先计算出的，对于市场价格频繁波动的情况并不适用，可能会造成 SLA 约束无法满足的情况。

从一个云服务中间商的角度考虑，Song 等人 \cite{song2012optimal} 提出了一个基于 Lyapunov 优化技术的考虑整体效益的动态竞价算法。在这个场景中，中间商接受来自云用户的作业提交请求，利用竞价型实例完成这些作业。所有作业请求都按顺序处理，这里对作业的类型限定为计算为主、可被中断，如：大数据分析（Hadoop作业）、科学计算批处理作业、生物数据处理等适合竞价型实例的。令第 $m$ 个作业的大小（即第 $m$ 个用户所需的计算量）为 $L_m$，其中 $L_m \leq L^{MAX}, \forall m$。这里 $L_m$ 是一个随机变量，但其概率分布不为中间商所知。定义 $T_m$ 为完成作业 $m$ 所需的时间，$C_m$ 为完成作业 $m$ 所需的成本。假设中间商向用户收取的费用为 $R(L_m)$，$R(.)$ 为一个任意的有届函数。则有中间商执行作业 $m$ 得到的单位时间净利润为 $(R(L_m) - C_m)/T_m$。中间商的最大化平均收益的问题可以形式化表达如下：
\begin{equation}\nonumber 
\max \lim_{M \rightarrow \infty}{\frac{\sum_{m=1}^M(R(L_m)-C_m)}{\sum_{m=1}^MT_m}}  
\end{equation}
s.t.
\begin{equation}\nonumber 
\lim_{M \rightarrow \infty}{\frac{\sum_{m=1}^MC_m}{\sum_{m=1}^MT_m}} \leq \alpha 
\end{equation}
and
\begin{equation}\nonumber 
\lim_{M \rightarrow \infty}{\frac{\sum_{m=1}^ML_m}{\sum_{m=1}^MT_m}} \geq \beta 
\end{equation}

Song 等人指出由于模型中的 $L_m$ 的概率分布无法确定，因此应考虑设计一个在线竞价策略解决该问题。基于排队论对这一服务模型进行分析，他们使用 Lyapunov 优化技术给出了一个近似最优的收益最大化动态竞价算法。该模型具有非常良好的扩展性，可以非常方便的加入用户的一些 SLA 约束，例如：用户要求作业在一个置顶的截止时间之前完成，则可以在模型中加入约束 $T_m < D_m$ 即可。其中，$D_m$ 为 作业 $m$ 的截止期限。

Song 等人的工作分析了竞价型实例的价格模型，从云服务中间商的角度给出了一个如何利用竞价型实例处理用户提交的作业请求实现收益最大化的模型和动态竞价算法，该模型还可以方便的扩展加入用户对作业截止时间的要求。这个工作在数学模型的层面上对竞价型实例的价格变化和计算成本与作业完成时间的权衡进行了深刻诠释。

\section{在竞价型云平台上实现高可用、高可靠}
\subsection{在竞价型云平台上提供高可用服务}
上一节介绍的所有研究工作都集中于时间灵活的、可中断的计算任务，这些计算任务的特点是可以轻松地切换到竞价型计算资源可用的时候再继续执行。这对于需要随时响应请求，不可中断的服务来说是不可能做到的。

最近，已经开始有研究者关注如何使用竞价型实例提供高可用服务这个问题。He 等人 \cite{He:2015:CCH:2749246.2749275}  提出了一个使用竞价型实例提供在线服务的解决方案。该方案大幅降低了提供在线服务的成本，通过使用嵌套虚拟化和虚拟机迁移技术来保证竞价型实例被回收时在线服务的可用性。

该方案需要在申请的竞价型实例上运行一个嵌套的虚拟机管理器（VM Hypervisor），以利用虚拟机迁移技术实现在线服务的无缝迁移。其使用的云平台上的嵌套虚拟化实现是 XenBlanket \cite{Williams:2012:XVO:2168836.2168849}，但由于没有底层虚拟机管理器提供的硬件虚拟化支持接口，XenBlanket 的运行开销很大。尤其是计算密集的程序，运行时开销至多达到 68\%。 运行在线服务的嵌套虚拟机需要定期执行内存检查点任务，将内存快照存储到网络接入的 EBS 存储卷上。利用竞价型实例被回收时的告警时间，可以将最后一次内存检查点之后的更新同步到持久化的 EBS 上。由于应急处理的时间只有两分钟，这个方案使用了一个时间可控的内存检查点机制 \cite{Singh:2013:YEG:2482626.2482642}。该内存检查点机制在给定可用时间限制 $\tau$ 的情况下，通过动态调节后台检查点执行频率来保证内存状态的更新量不会超过一个阀值，即能够在时间 $\tau$ 内将全部更新写到持久化存储。由于告警时间短暂，这个方案要求嵌套虚拟机在运行过程只使用持久化的 EBS。这对于需要高 IOPS 的在线服务来说是一个很严重的问题。另外为减少迁移过程中的停机时间，该方案还使用了延迟虚拟机恢复技术。

在竞价策略方面，He 等人提出了被动策略（reactive）和主动策略（proactive）两个竞价方式。被动策略将竞价设为同类型按需型实例的价格。当竞价型实例的价格超过了按需型实例的价格，必须立即申请一个新的按需型实例。当新的虚拟机实例启动后，运行在线服务的嵌套虚拟机被强制迁移到这个新申请的按需型实例上。主动策略则将竞价设为高于同类型按需型实例的价格（以 $k$ 倍于同类型按需型实例的价格设定竞价，$k > 1$）。当竞价型实例的价格超过了按需型实例但没有超过竞价时，可以申请一个按需型实例。在这个按需型实例启动后，可以按计划将运行在线服务的嵌套虚拟机迁移到按需型实例上。如果竞价型实例的价格直接超过了竞价，处理方式则和被动策略相同。主动策略的优势在于存在一定概率避免在竞价型实例被回收时进行强制迁移。当竞价型实例的价格重新降回按需型实例价格以下时，运行在按需型实例上的嵌套虚拟机可以按计划迁移回新申请的竞价型实例。在可用区的选择策略上，He 等人给出的策略是一个简单的贪心策略。该策略直接选择多个可用区中竞价型实例的市场价格最低的。

He 等人的工作对在竞价型云平台上提供在线服务进行了积极地尝试，为实现在线服务的可用性 SLA 给出了一个值得借鉴的思路。但这个方案存在强制迁移带来的服务不可用，嵌套虚拟化带来的大量性能开销，以及无法使用高性能的本地存储等问题。本文的竞价型实例上在线服务的轻量级暖备机制正是致力于解决该工作在服务可用性和性能上的巨大限制。

\subsection{将竞价型实例封装为可靠计算资源}
Sharma 等人 \cite{Sharma:2015:SDD:2741948.2741953} 站在云平台经销商的角度，从一个更基础的层面考虑可用性和可靠性。他们设计实现了一个基于竞价型云平台的导出云平台 SpotCheck，创造性地在不可靠的计算资源上向云租户提供可靠的计算资源。嵌套虚拟化技术和虚拟机迁移机制也被引入用于隐藏底层频繁的竞价型实例失效。

在保障可用性、可靠性上，SpotCheck 所使用的方法同 He 等人基本相同。所涉及到的技术也同样是：云平台上的嵌套虚拟化实现 XenBlanket，虚拟机活迁移技术、时间可控的内存检查点机制，以及延迟虚拟机恢复技术。

在架构上，SpotCheck 维护多个虚拟机实例资源池，通过嵌套虚拟机的形式向用户提供同样是 IaaS 形式的计算资源。在 SpotCheck 的多个虚拟机实例资源池中，有竞价型实例的资源池，也有按需型实例的资源池。另外，SpotCheck 还拥有一个由按需型实例组成的备份服务器池。备份服务器主要用于改善竞价型实例的内存检查点性能，内存检查点数据无需写到远程的磁盘上只需发送给备份服务器即可。通过使用备份服务器还有机会优化最终写入磁盘的数据量，减少大量不必要的 I/O 开销。多个竞价型实例复用一个备份服务器大幅降低了成本，使得利用备份服务器为竞价型实例提供高效的内存检查点机制变得可接受和实用。

还有一个需要多加考虑的是网络问题。当嵌套虚拟机发生迁移时，为了避免已经建立的网络连接不丢失需要同时实现网络 IP 地址的切换。由于底层虚拟机管理器不知道嵌套虚拟机的存在，嵌套虚拟机不能通过在网络中广播 ARP 包的方式通知他的新位置。通过在云平台的虚拟机实例中添加多块网卡并为每个网卡分配一个 EIP 地址，将不同 EIP 地址通过 NAT 机制映射到不同的嵌套虚拟机，SpotCheck 实现了嵌套虚拟机对外界的可见。在嵌套虚拟机进行迁移时，SpotCheck 通过 Amazon API 将 EIP 地址从原来的宿主虚拟机切换到新的宿主虚拟机，同时维护嵌套虚拟机管理器的 NAT 配置。

对于一个用户申请虚拟机实例的请求，SpotCheck 可以简单的在底层云平台中申请一个同样类型的竞价虚拟机实例，配置一个嵌套虚拟机提供给用户。也可以考虑申请一个配置类型更大的虚拟机，在其上运行嵌套虚拟机提供给多个用户。默认情况下，SpotCheck 使用一个贪心的策略申请竞价型实例。如果单位计算能力最便宜的实例类型恰好是用户请求的类型，则直接申请该类型。如果存在单位计算能力最便宜但实例类型更大的竞价型实例则申请该类型在其上启动一个用户需要的配置类型的嵌套虚拟机，剩余的计算能力用于提供给之后的用户申请。另外一个策略是考虑价格的稳定性，一个稳定的可用区和竞价型实例类型可以减少被回收的风险，因而减少强制迁移从而提高嵌套虚拟机的可靠性。

另外，SpotCheck尽量将用户的虚拟机请求分散到多个虚拟机实例资源池中。因为各个虚拟机实例资源池是独立的，这能减少在某个虚拟机实例资源池被回收时嵌套虚拟机迁移的数量。在为嵌套虚拟机设定备份服务器时，SpotCheck 尽量将同一个虚拟机实例资源池的嵌套虚拟机分配给不同的备份服务器。因为在某个虚拟机实例资源池被回收时，如果其中大量的嵌套虚拟机使用相同的备份服务器将对其造成巨大的请求压力。SpotCheck 使用了简单的 round-robin 策略进行备份服务器分配。在竞价的策略上，SpotCheck 同 He 等人的被动策略和主动策略相同。

SpotCheck 提供了一个非常创新的利用竞价型实例的思路，通过对底层竞价型实例的封装并使用一系列高可用、容错机制，实现了对竞价型实例的节点失效的隐藏得以向用户提供可靠计算资源。由于云平台上的嵌套虚拟化的缺陷以及回收告警时间的限制，SpotCheck 在系统运行时开销和维护可用性上的计算成本开销都比较大。尽管有一定的局限性，Sharma 等人的工作仍有很大意义，为竞价型云平台的相关研究提供了有益的参考和启发。

\section{大规模并行任务调度}
分布式系统中的并行任务分配与调度策略是一个被广泛研究的经典问题。在不同的场景和多变的需求下，早期的研究者们提出了各种各样的调度方法和系统设计。TAGS \cite{balter} and SITA-V
\cite{Crovella:1998:TAD:277851.277942} 解决了在集群中处理诸如 HTTP 请求的众多服务器节点的独立任务调度问题。这两个工作的目标是较低的服务平均响应时间和较短的服务停滞时间，二者均认为问题的关键是负载不均衡导致的任务大小重尾分布带来的挑战。Manoharan等 \cite{Manoharan:2001:ETD:373047.373064} 分析了任务复制（Task duplication）在海量并行任务处理中的有效性，TDS \cite{rohtua} 和一些其它工作 \cite{ahmad, Dogan:2002:LDB:850943.853100} 通过在最大并发和最小任务间通信权衡尝试最小化调度队列长度和调度时间。Silberstein 等人 \cite{silberstein} 提出了针对海量并行短任务组成的异构动态负载的调度策略。通过考虑通信开销和异构计算环境，Ahemad 等人 \cite{Ahmad:1991:SLB:126283.126284} 和 Uçar 等人 \cite{ucar} 分别提出了一些动态任务调度方法。这些工作是一些代表性的早期研究成果，本文的工作着眼于提升竞价型云平台中大规模计算密集型并行任务所面临的异常者问题，同这些早期工作有着不同的背景和需求。

\subsection{出现异常者的原因、普遍性及影响}
在大规模分布式计算系统中，随着并发数量的不断加大，资源竞争、节点的异构性、任务分配不均衡、硬件或软件错误等因素的存在，经常导致少量的任务执行严重的落后于其它任务成为异常者。异常者的存在是一个普遍且影响严重的问题。Ananthanarayanan 等人 \cite{Ananthanarayanan:2010:ROM:1924943.1924962} 通过大量的集群运行日志数据详细地分析了异常者出现的原因，并对异常者的普遍性、严重性进行了量化分析。

以任务执行时间超过平均执行时间的 1.5 倍为标准判定异常者，Ananthanarayanan 等人的统计结果显示有 25\% 的作业中有超过 15\% 的异常者，有 1\% 的任务需要重新执行。需要重新执行是因为任务输出数据丢失，而依赖该输出结果的任务仍在等待该任务的执行结果。约 80\% 的运行时异常者所需的执行时间少于任务平均执行时间的 2.5 倍，这些任务的完成时间均匀分布在任务平均执行时间的 1.5 倍到 2.5 倍之间。剩下的异常者任务执行时间有明显的长尾特征。最严重的 10\% 的异常者完成任务需要的时间超过了正常节点的 10 倍，极大地拖慢了作业完成进度。

在异常者的影响上，根据他们的数据分析结果显示：如果没有执行缓慢的异常者的出现，作业完成时间可平均减少 15\%。如果既没有执行缓慢的异常者也没有需要重新执行的异常者，作业完成时间可平均减少超过 34\%。需重新执行的异常者出现的频率远远小于执行缓慢的异常者，但一旦出现，他们对作业完成时间的影响会更大。

\subsection{投机执行、任务复制及微任务理念}
异常者的出现极大地拖慢了大规模并行任务的作业完成时间，很多研究者尝试解决由硬件或软件错误、任务倾斜或节点异构、资源竞争等造成的异常者问题。他们的研究工作中解决问题的思路主要包括：
\begin{enumerate}
\item 利用空闲节点对潜在的异常者任务进行投机执行，减少异常者对作业执行带来的影响
\item 通过对每个任务同时执行多个副本来实现对异常者的容错
\item 通过实现细粒度任务划分和高效、低开销的调度从计算框架层面消除节点计算能力差异带来的异常者问题
\end{enumerate}

广义的投机执行（Speculative Execution）思想在计算机系统结构领域有着相当长时间的历史，被广泛应用在处理器流水线分支预测、操作系统中进程的投机执行、数据库事物的乐观并发控制等系统领域的各个层面。投机执行思想的核心是如果系统中有额外的资源可用，可以利用它做一些即使最终是无用的操作。这可以减少在确认了该操作有用时再执行的延迟，只要没有对外可见的结果该类无用操作是无害的。

最初的大规模并行计算领域的投机执行相关设计见于 MapReduce 计算框架：在一个MapReduce作业大部分任务已经完成的时候，Master节点针对还没有完成的任务启动一个对应的备份任务（Backup Task）。只要该任务的初始执行或备份执行有一个完成，就标记该任务完成。这一简单的手段为MapReduce作业的执行带来了 44\% 的性能提升\cite{dean}。然而在这个设计中，备份任务执行时已经有任务完成。即使备份任务正常执行完且早于对应的主任务，作业进度依然被拖慢了一倍。这个设计主要针对硬件、软件错误类的节点异常情况，出现这类错误时任务可能完全无法继续执行或数十倍的慢于正常节点（如：一个损坏的磁盘由于频繁修正校验错误导致性能从30 MB/s 降到 1 MB/s）。

在没有待运行任务的情况下，Hadoop 也会利用空闲节点进行投机执行。Hadoop 使用的投机执行策略更进一步的引入了对任务进度的估计，通过监控任务执行给每个任务一个 0 到 1 之间的进度值。对于 Map 任务根据输入数据读取的比例确定其进度值。对于 Reduce 任务，执行被分为三个阶段，每个阶段给予 $1/3$ 的分值：
\begin{enumerate}
\item 拷贝阶段：Reduce 任务获取 Map 的输出；
\item 排序阶段：将Map 的输出根据键名（key）排序；
\item Reduce 阶段：对每个键名对应的 Map 输出列表应用用户定义的 Reduce 函数。
\end{enumerate}

在每一个阶段，分值按数据处理的比例确定。Hadoop 将少于同类任务（Map 或 Reduce）的平均分值 0.2 分且执行时间超过一分钟的标记为异常者。对于异常者任务认为他们是同样缓慢的，进行投机执行时会保证只有一个该任务的投机执行副本。对于同构计算环境中的 Hadoop 集群，这个异常者检测方法是有效的。因为一个 MapReduce 作业中的任务通常是大致相同的时间开始执行。
                                                                                                                                                                                                                                                                                                                                                                                       
Zaharia 等人 \cite{Zaharia:2008:IMP:1855741.1855744} 针对 Hadoop Scheduler 对 Reduce 任务进度预测的不合理、任务启动时间不完全相同，以及异构集群中节点性能差异、任务进度在不同时间段的速率差异等问题对投机执行的策略做出了改进，提出了 LATE (Longest Approximate To End) 投机执行策略。该策略的核心思想是优先考虑当前预测下最后完成的任务进行投机执行，显然这样才可能最大程度的缩减作业完成时间。Zaharia 等人用 $ProgressScore / T$ 表示任务执行速率 $ProgressRate$，其中 $ProgressScore$ 代表进度值，$T$ 代表任务已经执行的时间。基于一般 Hadoop 任务执行过程中进度速率大致保持一致的前提，任务剩余完成时间可用 $\frac{1 - ProgressScore}{ProgressRate}$ 估测。

LATE \cite{Zaharia:2008:IMP:1855741.1855744} 还包含一些启发式的策略：首先通过设定阀值 $SlowNodeThreshold$ 保证投机执行的任务不分配到慢节点上，然后通过阀值 $SpeculativeCap$ 控制用于投机执行的计算资源，最后通过 $SlowTaskThreshold$ 避免了对进度较快任务的无效投机执行。相较 Hadoop Scheduler，LATE 优势主要在于：
\begin{enumerate}
\item 在异构集群中表现更稳定。因为 LATE 按照对作业完成时间的影响大小区分慢任务的投机执行优先级，同时限制了投机执行的任务数避免造成共享资源竞争。
\item 在选择使用哪个节点进行投机执行时，LATE 考虑了节点异构性，会避免在一个慢节点上执行投机任务。
\item 通过预测任务完成剩余所需时间而不是任务的执行速率，LATE 保证了每个投机执行的任务都对减少作业完成时间有帮助。例如：任务 $A$ 执行速率是平均速率的 $1/5$，但执行进度为 90\%；另一个任务 $B$ 执行速率是平均速率的 $1/2$，但执行进度只有 10\%。显然选择任务 $B$ 投机执行时更好的选择。
\end{enumerate}

Ananthanarayanan 等人 \cite{180304} 进一步研究了数据中心大量的交互式数据分析作业，发现这些作业同样深受严重拖慢整体进度的异常者问题影响。即使应用了向 LATE \cite{Zaharia:2008:IMP:1855741.1855744} 和 Mantri \cite{Ananthanarayanan:2010:ROM:1924943.1924962} 这样的针对异常者的大规模并行任务调度方法，这些延迟敏感的作业仍然存在执行时间数倍于平均任务执行时间的异常者。这是因为投机执行的方式对于这类交互式任务存在根本性的缺陷，投机执行需要一段时间收集任务性能的统计采样信息从中发现异常者然后才可能执行一个新的任务副本。缺少足够敏捷性的投机执行策略对于这类任务明显力不从心。对于交互式任务，最好的容错方式是任务复制即同时执行多个相同的任务副本。Ananthanarayanan 等人称之为 ``任务克隆''，通过同时执行任务的多个副本在概率上减少了异常者任务出现的机会。因为同一个任务所有副本都是异常者才会造成该任务拖慢整个作业，而且同时执行的任务副本也不存在投机执行面对这类任务时的无效问题。虽然任务克隆的计算资源开销很大，但在集群中交互式任务所占用的资源很少因而任务克隆增加的计算资源开销可以接受。假设一个交互式作业有 $n$ 个并行的任务，集群中出现异常者的概率为 $p$，该作业出现异常者的可接受风险为 $\epsilon$，则需要进行任务克隆的副本数为：
\begin{equation} \nonumber
c = \lceil \log(1-(1-\epsilon)^{(1/n)})/\log p \rceil
\end{equation}

令 $C$ 表示集群的计算资源总量，$U$ 表示集群的计算资源使用率，$\beta$ 表示可用于任务克隆的计算资源预算，$B_U$ 表示任务克隆已使用的计算资源量，$\tau$ 表示保证集群正常运行的资源使用率上限阀值。一个可接受的任务克隆，只需保证 $(B_U + c \cdot n \leq (C \cdot \beta)$ 和 $(U + c \cdot n) \leq \tau$。

任务克隆的主要挑战是在整个工作流的执行中多个任务副本读取中间数据时产生的竞争。如果让多个任务副本都读取上一个任务最快的副本输出的中间结果，由于数据读取上的 I/O 竞争必然拖慢了当前任务所有副本的执行。如果让多个任务副本读取上一个任务不同的副本输出的中间结果，则各个任务副本在执行上并非同步，这也就失去了任务克隆的意义。这两种方式的不足实际上都是无法区分正常执行的任务和异常者任务：前者假设除了第一个完成的任务剩下的都是异常者，后者假设所有任务都是正常的。Ananthanarayanan 等人给出了一个混合的方法————延迟分配：首先在拿到上一个任务的最快副本输出的中间结果后，立即执行一个任务副本；然后等待 $\omega$ 时间，如果其他任务副本仍得不到可以独占的中间结果数据，剩下的所有任务副本同时执行，使用已完成的一个中间结果数据。决定延迟分配效果的最关键部分是 $\omega$ 的选取。确定 $\omega$的主要步骤如下：
\begin{enumerate}
\item 计算任务克隆读取中间数据的预期时间。假设中间数据的大小为 $r$，没有竞争下的读带宽为 $B$，有竞争时的读带宽为 $\alpha B, \alpha \leq 1$。如果一个任务克隆没能独占一个中间数据而是与其他任务克隆竞争，则第一个读取中间数据的任务克隆的预期时间 $T_C$ 为 $(\omega + (\frac{r - B \omega}{\alpha B}))$，其他任务克隆的预期时间 $T_C$ 为 $(2 \omega + (\frac{r - B \omega}{\alpha B}))$。如果一个任务克隆拿到了独占的中间数据，而其他任务克隆也拿到了各自的中间数据，则第一个读取中间数据的任务克隆的预期时间为 $\frac{r}{B}$，而其他的任务克隆的预期时间为 $\frac{r}{B} + min(\frac{r}{B}, \omega)$。令 $p_c$ 表示一个任务克隆能拿到独占中间数据的概率，则任务克隆读取中间数据所需时间期望为 $p_c T_C + (1-p_c) T_E$。
\item 用所有任务克隆的读取中间数据所需时间来估测任务克隆执行所需时间。一个任务克隆可能需要读取多个中间数据，根据这些预估时间期望加上计算用时的期望就可以算出一个任务克隆的预计完成时间 $T_i$，任务的预期执行时间是所有任务克隆的最小值，即 $min(T_i)$。
\item 找出使上一步中的任务预期执行时间最小化的 $\omega$。通过对已完成作业采样 $B$，$\alpha$，$p_c$，以及任务完成时间等值，可以发现 $B$ 同节点上同时进行的I/O流数量有关，$p_c$ 和 $w$ 成反比例关系。基于这两点，可以选出最小化任务预期执行时间的 $\omega$。$\omega$ 定期更新，且根据作业中的任务数自动调整。
\end{enumerate}

针对使用广泛且特殊的交互式作业类型，Ananthanarayanan 等人指出了投机执行策略的不够灵敏，并设计了更为激进的任务克隆机制来使交互式的作业免受异常者任务的影响。对大规模任务并行调度算法提供了有益的补充。

观察到 Dremel \cite{36632}，Spark \cite{Zaharia:2010:SCC:1863103.1863113} 等大规模数据分析框架更低延迟更大规模并行的发展趋势，Ousterhout 等人 \cite{Ousterhout:2013:CTT:2490483.2490497} 提出了一个新的任务调度理念，认为应该将计算集群中数据并行的作业拆分为更细粒度的微任务，每个微任务可以在 1 秒以内（数百毫秒）完成。微任务提供了极高的调度灵活性，从根本上免除了复杂的消除异常者任务的机制和技术。通过将作业拆分为数百万个微任务，任务调度器可以根据计算节点状态均匀的分配任务。无论是任务分配不均、还是节点差异，资源竞争，抑或是硬件或软件错误导致的节点异常，在微任务粒度下调度器有足够的灵活性调整任务调度避免作业执行被拖慢。另外，微任务的形式还可以解决交互式任务在当前集群中存在的等待时间过长问题，让批处理作业和交互式作业更好的共享计算资源。

虽然使用更小的任务粒度有很大优势，但要实现通用的微任务调度设计存在着如下挑战：
\begin{enumerate}
\item 实现极低的任务启动开销，以 100 毫秒的任务执行时间和 1\% 的启动时间开销计算，要求任务启动时间要达到 1 毫秒。
\item 需要高可扩展的存储系统以支持微任务对小数据块的读写，大量对相同数据块的并发读写需要分布式的元数据管理，这是传统分布式文件系统，如：HDFS \cite{hadoop}，无法满足的。
\item 实现极高吞吐的任务调度，以在一个有 160000 核的集群（10000 个 16 核的机器）中并行 100 毫秒的微任务为例，需要调度器每秒调度 1600000 个任务。
\item 保证所有的作业都支持微任务，这需要编程模型上的一些改进，但仍会有无法切分的任务存在。
\end{enumerate}

Ousterhout 等人设计了一个解决上述挑战，支持微任务模型的调度器 Sparrow \cite{Ousterhout:2013:SDL:2517349.2522716}。Sparrow 采用去中心化的设计，保证了微任务模型下极强的扩展性和可用性。Sparrow 在并行任务调度上主要应用了随机化负载均衡技术。基本的随机化负载均衡技术探测两个随机选取的服务器并将一个新任务调度到队列长度较短的服务器上。由于微任务的并发量巨大，这个简单的随机化负载均衡方法无法满足效率上的要求。Sparrow 应用了改进的多选择随机化负载均衡方法，采用批量采样的方式每次可以随机选择 $d \cdot m$ 个（$d \geq 1$）服务器节点，调度一个作业的 $m$ 个任务。批量采样保证了调度性能不会随着作业并行度的上升而下降。Sparrow 还采用了延迟绑定机制在工作节点准备好运行一个任务时才将其分配到该节点，避免使用任务队列长度不能准确反映负载和各个调度器同时采样的竞争导致的消息延迟对性能的影响。另外，Sparrow 还在节点上提供多队列以执行全局策略，并支持数据分析框架需要的作业及任务放置约束。

\section{分布式系统可用性}
在分布式系统中，无论是在节点和网络不可靠的情况下保证系统运行的可靠性 \cite{Lamport197895}，抑或是保证多个数据副本的一致性 \cite{Gifford:1979:WVR:800215.806583}，还是实现互斥锁（Mutual Exclusion）访问临界区资源 \cite{Garcia-Molina:1985:AVD:4221.4223}，通常需要借助于一些使分布式系统中的节点对限制性操作达成一致的机制。Leslie Lamport \cite{Lamport197895} 提出了预先定义出可以进行限制性操作的组集（Set of Groups）的方法，只要任何两个组都存在共同的节点就可以保证互斥性。David K. Gifford \cite{Gifford:1979:WVR:800215.806583} 在分布式数据副本控制中设计了给每个节点分配投票权的 Quorum 投票机制。Quorum 投票机制要求在分布式系统中一个操作的执行必须得到一组拥有大多数投票权的节点的同意。因此当一个分布式系统中的一些节点因为网络或软件/硬件错误而失效不可用时，这个分布式系统可能因为没有任何可以进行限制性操作的组集或没有足够的投票权而无法继续运行。用 CAP定理 \cite{Fox:1999:HYS:822076.822436} 来解释就是：在必须容忍网络分区（Partition Tolerance）的情况下，一个分布式系统必须在可用性（Availability）和 一致性（Consistency）之间做出权衡，不可能同时满足。为了保证分布式系统的正确性，只能在可用性上做出牺牲。当然只要可用的节点仍然能形成进行限制操作的组集或拥有足够的投票权，分布式系统可以容忍一定数量的节点失效保持可用。

综上，一个分布式系统的可用性是由组成它的各个节点的可用性以及预定义的组集设置或各个节点的投票权决定的。Garcia-Molina 等人 \cite{Garcia-Molina:1985:AVD:4221.4223} 对预定义组集和 Quorum 投票机制进行了分析，证明了看似相同的组集设置和投票权分配并不完全等价。存在没有任何投票权分配可以对应的组集设置，Quorum 投票机制的表达空间只是预定义组集的一小部分。他们还指出在不超过 5 个节点的情况下二者在最优选择上是等价的，并讨论了如何在巨大的状态空间中通过部分枚举策略找出较优的不受涵盖法团（Non Dominated Coterie）。Tang 等 \cite{Tang:1989:SPS:67544.67809} 则引入了类似的可接受集的概念，除了一些明显不可取的组合，这同前者是等价的。

对于一个使用 Quorum 投票机制的分布式系统，最简单的投票权分配方式是给每个节点相同的投票权。对于节点失效概率相同的情况这样分配是合理的，但是对于节点失效概率不同的情况则会影响分布式系统的可用性。很多研究工作对在各种不同的场景下如何分配投票权给出了相应的解决方案。Tong 等人 \cite{25789} 首先给出了实现最优投票权分配的条件，然后提出了在理想网络条件下的最优投票权分配算法和效率更高但是次优的投票分配算法，最后在一些非理想网络的情况下讨论了如何分配投票权。Spasojevic 等人 \cite{262589} 在操作独立的情况下改进了 Tong 等人的方法，将投票分配算法从指数级复杂度降到了线性复杂度，并证明了只有在全联通网络中该分配算法才是最优。Peleg 等人 \cite{Peleg1995210} 研究了 Quorum 系统，尤其是不受涵盖法团系统的失效概率，并展示了带权重的投票方法保证渐进高可用性的条件。Amir 等人 \cite{Amir1998223} 刻画了在一般情况下（节点失效概率可以是 0 到 1 区间中的任何值）的最优可用性的 Quorum 系统，并讨论了节点失效概率未知但可以估计的实际场景，最后给出了一个根据估测的节点失效概率计算近似最优的 Quorum 系统的鲁棒且有效的投票权分配算法。

\section{高可用设计与容错机制}
在计算机系统设计中，高可用是设计者追求的一个目标。高可用系统设计要求：消除组件的单点失效、保证连接部分的可靠、在失效发生时能够检测到。在计算机系统中检测失效一般是容易实现的，组件和连接部分的单点失效则需要相应的容错机制。冗余容错是计算机系统中避免单点失效的非常常见和重要的手段。

在机器节点的层面，双机备援是提供高可用服务的一种普遍做法。根据备援的状态和数据的同步方式又可分为：冷备（Cold Standby）、暖备（Warm Standby）、热备（Hot Standby）、双活（Active-Active）。其中，1）冷备是指备援节点在主节点失效时才被启动并进行必要的配置和数据恢复，发生故障后通常需要几小时的恢复时间。2）暖备是指软件和服务在备援节点已安装，备援节点定期地同主节点同步数据。当发生故障时，通常需要几分钟的恢复实现。3）热备是指软件和服务在主节点和备援节点上都已安装且可用，但备援节点不处理数据和用户请求。数据近乎实时的镜像到备援节点，发生故障后的恢复时间只需几秒钟。4）双活则是指两个节点都可用，并行地处理请求。数据同步在两个节点上双向进行，在发生故障时可以实时恢复。另外，在实际使用中两个节点可以在逻辑上互相作对方的备援节点，这被称为``互备''（Standby Pair）。互备的两个节点可以运行相同的应用或服务（类似于双活），也可以运行不同的应用或服务。

在数据存储层面，RAID \cite{Patterson:1988:CRA:50202.50214} 是提供存储可靠性的通用做法。RAID-1 (disk mirroring) 以及多副本更是实现存储高可用的基本方式。双机高可用设计中通常使用 DRBD \cite{DRBD:2015} 等类似于网络 RAID-1 的技术实现数据存储的同步。DRBD 提供了两个节点间通过网络复制磁盘数据的方式，支持同步和异步多种镜像模式。分布式文件系统 GFS \cite{Ghemawat:2003:GFS:945445.945450}、HDFS \cite{Hadoop} 等则使用多副本技术（一般配置为三副本）实现高可用，HDFS 的副本放置策略一般为：将第一个副本放在本地节点，将第二个副本放到本地机架上的另外一个节点，而将第三个副本放到不同机架上的节点。当然这些技术不止提高了可用性，也实现了性能提升，负载均衡，可靠性增强等。

在故障发生时为了避免丢失正在处理中的用户请求和数据，往往还需要在向备援节点同步内存状态以实现平滑的故障转移（Failover）。实现内存状态同步一般需要借助内存活迁移（虚拟机迁移 \cite{Clark:2005:LMV:1251203.1251223} 或进程级迁移 \cite{Wang:2008:PPL:1413370.1413414} 等）或是内存检查点（Checkpointing）及恢复（Restore）的方式 \cite{Duell03thedesign, CRIU:2016}。

活迁移的策略上主要包括 Pre-copy 和 Post-copy 两类方法。Pre-copy 的内存迁移方式包括两个阶段：热身（Warm-up）阶段和停机－拷贝（Stop-and-copy）阶段。在热身阶段，虚拟机继续在源节点正常运行而虚拟机管理器（Hypervisor）从源节点向目的节点拷贝全部内存页，在拷贝过程中被修改的内存页会再次被拷贝直至收敛（再次拷贝的速率和内存页更改速率相当）。在热身阶段结束后，源节点上的虚拟机将被停止，剩余脏内存页被拷贝到目的节点，最后虚拟机在目的节点上恢复执行。在停机－拷贝阶段中，在停止和恢复执行之间的这段时间叫 ``停机时间''。停机时间根据虚拟机上应用的内存工作集大小一般在几毫秒到几秒之间。Post-copy 的内存迁移方式则是立即挂起源节点上的虚拟机，将虚拟机执行状态的最小子集（CPU 状态、寄存器值、不可换页的内存）传输到目的节点。然后虚拟机就在目的节点恢复执行，同时源节点不断的向目的节点同步剩余的内存页（预先调页）。当目的节点上的虚拟机试图访问一个还没有传输过来的内存页时，操作系统会生成一个页错误然后陷入内核态最后从源节点获取所需内存页。页错误过多会对虚拟机中运行的应用性能带来严重影响，但每个内存页只传输一次。而 Pre-copy 方法可能对一个内存页传输多次。Pre-copy 的优势在于在源节点保存了内存的最新状态，当迁移过程中目的节点发生故障时可以在源节点恢复虚拟机，Post-copy 则无法恢复。

在内存检查点技术上，Singh 等人 \cite{Singh:2013:YEG:2482626.2482642} 针对有时间限制的场景提出了时间可控（Bounded）的内存检查点方法。该方法通过动态调节后台检查点过程可以在给定的时间上限 $\tau$ 内完成一次检查点，核心是保证增量的内存更新不超过一个阀值因而可以在给定网络带宽给定时间 $\tau$ 内安全的传输完成。

此外，状态机复制（State Machine Replication） \cite{Lamport:1984:UTI:2993.2994, Schneider:1990:IFS:98163.98167} 也是一种常见的容错技术。通过将服务器抽象为确定性状态机并在多个服务器副本之间以相同顺序执行相同操作，状态机复制技术可以在部分服务器副本出现故障时继续提供服务。一个确定性状态机包括：
一个状态集合、一个输入集合、一个输出集合、一个转移函数（输入 $\times$ 状态 $\rightarrow$ 状态）、一个输出函数（输入 $\times$ 状态 $\rightarrow$ 输出）、一个唯一的起始状态。状态机从起始状态开始，每一个输入经过转移函数和输出函数生成一个新的状态和输出，没有输入则状态机保持状态不变。确定性是指多个相同的状态机从起始状态开始接受相同输入且顺序相同将到达相同的状态并产生相同的输出。确定性提供了容错的保证，出现错误的副本必然在状态和输出有别于其他正常节点。多个服务器副本间需要通过一致性协议，如：Paxos \cite{Lamport:1998:PP:279227.279229} 决定是否执行一个操作，协调与客户端的交互。一个最简单的例子是有三个副本的情况，可以容忍任意一个节点出错。如果有两个节点出错，则无法判断哪个状态是正确的。一般来说，支持 $F$ 个节点失效需要有 $2F + 1$ 个副本。